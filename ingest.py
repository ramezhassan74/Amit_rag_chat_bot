import os
import pdfplumber   # Ù…ÙƒØªØ¨Ø© Ù‚ÙˆÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ PDF
import gradio as gr

# Ø­Ø· Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù€ docs
DOCS_DIR = r"D:\Chatbot\AMIT_RAG_Chatbot\docs"

def read_pdf(path):
    text = ""
    try:
        with pdfplumber.open(path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:  # ÙŠØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„ØµÙØ­Ø© Ù…Ø´ ÙØ§Ø¶ÙŠØ©
                    text += page_text + "\n"
    except Exception as e:
        print(f"âš ï¸ Error reading {path}: {e}")
    return text


def chunk_text(text, chunk_size=500, overlap=50):
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + chunk_size
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks


def ingest_docs():
    all_chunks = []

    if not os.path.exists(DOCS_DIR):
        print(f"âŒ Directory not found: {DOCS_DIR}")
        return []

    docs_files = os.listdir(DOCS_DIR)

    for file in docs_files:
        path = os.path.join(DOCS_DIR, file)
        text = ""

        if file.lower().endswith(".pdf"):
            print(f"ğŸ“„ Processing: {file}")
            text = read_pdf(path)
            if text:
                chunks = chunk_text(text)
                all_chunks.extend(chunks)

    print(f"âœ… Loaded {len(all_chunks)} chunks from {len(docs_files)} files (PDF only)")
    return all_chunks


if __name__ == "__main__":
    chunks = ingest_docs()
    print(chunks[:2])

    